{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "np.set_printoptions(precision=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some theory\n",
    "\n",
    "The goal of using scaled dot product attention in the original transformer paper is that if the entires in $a$ and $b$ are i.i.d. random variables with mean $\\mu=0$ and standard deviation $\\sigma$, then their scaled dot product (scaling by $1/\\sqrt{N}$) will also have mean 0 and standard deviation $\\sigma$). This helps keep training stable. Can we make this happen in the case where $\\mu\\neq 0$?\n",
    "\n",
    "Suppose our vectors have entries that are i.i.d. with mean $\\mu$ and standard deviation $\\sigma$. The dot product will have mean:\n",
    "$$\n",
    "\\mu'=N\\mu^2\n",
    "$$\n",
    "\n",
    "and the variance will be\n",
    "\n",
    "$$\n",
    "(\\sigma')^2 = N[\\sigma^4+2\\sigma^2\\mu^2]\n",
    "$$\n",
    "\n",
    "If $\\mu=0$, this reduces to $(\\sigma')^2=N\\sigma^2$, so scaling the dot product by $1/\\sqrt{N}$ does what we want. However, if $\\mu\\neq 0$, then we would need to scaled by a different value, $K$, satisfying:\n",
    "\n",
    "$$\n",
    "\\mu=N\\mu^2/K\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "\\sigma^2 = N\\sigma^2[1+2\\mu^2]/K^2\n",
    "$$\n",
    "\n",
    "These reduce to\n",
    "\n",
    "$$\n",
    "K=N\\mu\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "K^2 = N[1+2\\mu^2]\n",
    "$$\n",
    "\n",
    "In particular, this shows that $\\sigma$ is not independent of $\\mu$ and $N$:\n",
    "\n",
    "$$\n",
    "N^2\\mu^2=N[1+2\\mu^2]\\Rightarrow N\\mu^2 = 1 + 2\\mu^2 \\Rightarrow \\mu^2[N-2]=1\\Rightarrow \\mu = 1/\\sqrt{N-2}\n",
    "$$\n",
    "\n",
    "So, if for example, we choose elements from a uniform distribution $[0,2\\mu]$, then the standard deviation is $\\sigma=\\frac{\\mu}{\\sqrt{3}}$ and so we get the condition:\n",
    "\n",
    "$$\n",
    "\\mu/\\sqrt{3}=2/(N-1/\\mu^2)\n",
    "$$\n",
    "\n",
    "which simplifies to \n",
    "\n",
    "$$\n",
    "\\mu = (\\sqrt{3}+\\sqrt{3+N})/N\n",
    "$$\n",
    "\n",
    "So, if $N$ is very large, we want to have $\\mu\\approx 1/\\sqrt{N}$ and $\\sigma\\approx 1/\\sqrt{3N}$. And we would set $K=\\sqrt{N}$ just like in the normal case.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:  0.032987198755364266\n",
      "Standard deviation:  0.019045168081221246\n",
      "Dot product mean:  0.03299040589923298\n",
      "Dot product standard deviation.:  0.0009077264718225711\n"
     ]
    }
   ],
   "source": [
    "# Test of the above derivation (for the case of uniformly distributed values)\n",
    "\n",
    "N = 1024\n",
    "mu = (np.sqrt(3)+np.sqrt(3+N))/N #1/np.sqrt(N)\n",
    "sigma = (1+np.sqrt(1+N/3))/N #1/np.sqrt(3*N)\n",
    "print(\"Mean: \",mu)\n",
    "print(\"Standard deviation: \",sigma)\n",
    "samples = []\n",
    "for i in range(100000):\n",
    "    k = 2*mu*np.random.random((N,))\n",
    "    q = 2*mu*np.random.random((N,))\n",
    "    samples.append(np.dot(k,q)/(np.sqrt(3)+np.sqrt(3+N))) #samples.append(np.dot(k,q)/np.sqrt(N))\n",
    "\n",
    "# Print mean and standard deviation\n",
    "print(\"Dot product mean: \",np.mean(samples))\n",
    "print(\"Dot product standard deviation.: \",np.std(samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    y = np.exp(x)\n",
    "    return y/np.sum(y)\n",
    "\n",
    "\n",
    "def ExpBernoulliAlt(alpha,lam,size=None,method='binomial',N=1000): # returns True with probability $e^{alpha-lam}$\n",
    "    if size is None: \n",
    "        size = (1,)\n",
    "    if type(size) is int:\n",
    "        size = (size,)\n",
    "    elif type(size) is not tuple:\n",
    "        raise TypeError(\"size must be int or tuple\")\n",
    "    \n",
    "    if method == 'poisson':\n",
    "        M=np.random.poisson(lam=lam,size=size) # Get integers from poisson(lam).\n",
    "    elif method == 'binomial': # N is number to flip\n",
    "        M=np.random.binomial(N,lam/N,size=size)\n",
    "\n",
    "    spikes = np.random.random(size)<np.power(alpha/lam,M) # Spike if all K trials are successful\n",
    "    return spikes\n",
    "\n",
    "# def spikingMatMul(W,x,pop_size,n_steps,comb_method='OR'):\n",
    "#     n_tokens,n_embed = x.shape\n",
    "#     x_spikes = np.random.random((pop_size,n_steps,n_embed,1))<x\n",
    "#     W@x_spikes\n",
    "\n",
    "# If the value in q is s, then the corresponding probability will be s/N\n",
    "\n",
    "def approx_dot(q,K,pop_size,n_steps,comb_method='OR'):  # Represent value as population sum of spikes\n",
    "    n_embed,n_keys = K.shape\n",
    "\n",
    "    # Create spike arrays based on the values in q and K\n",
    "    q_spikes = np.random.random((pop_size,n_steps,n_embed,1))<q \n",
    "    K_spikes = np.random.random((pop_size,n_steps,n_embed,n_keys))<K\n",
    "    scaling_spikes = np.random.random((pop_size,n_steps,n_embed,n_keys)) < 1/np.sqrt(n_embed) # K x N matrix of boolean mask\n",
    "    \n",
    "    attn_score_spikes = q_spikes & K_spikes & scaling_spikes # AND\n",
    "\n",
    "    # Summing works better, but OR can work well if input rates are low\n",
    "    if comb_method == 'OR':\n",
    "        attn_score_spikes = np.any(attn_score_spikes, axis=2) # OR\n",
    "    elif comb_method == 'SUM':\n",
    "        attn_score_spikes = np.sum(attn_score_spikes, axis=2) # SUM\n",
    "    else:\n",
    "        raise ValueError(\"comb_method must be 'OR' or 'SUM'\")\n",
    "    # approx_attn_scores = np.mean(attn_score_spikes,axis=0) # Get spike rates\n",
    "    return attn_score_spikes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Computing the scaled dot product between two vectors\n",
    "\n",
    "Query: $q$\\\n",
    "Key: $k$\\\n",
    "Scaled dot product: $q\\cdot k/\\sqrt{N}$\n",
    "\n",
    "\n",
    "Basic method: \n",
    "\n",
    "* Create spikes trains with rates proportional to the elements of $k$, $q$ and $1/\\sqrt{N}$ (where $N$ is the lenght of $q,k$).\n",
    "* Use AND gates to combine this, yields spike trains with rates proportional to $q_ik_i/\\sqrt{N}$.\n",
    "* Sum these spike trains together. If each individual spike train is sufficiently sparse, we can approximate this with an inclusive OR.\n",
    "\n",
    "Challenges: \n",
    "* What if we want to using key or query vectors that are not in $[0,1]^N$?\n",
    "* What if input spike trains are not very sparse? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact scaled dot product:  0.00913\n",
      "Approximation using spikes:  0.00906\n",
      "Standard deviation:  0.00287\n",
      "Percent error:  0.81347 %\n",
      "Angle:  0.652  radians\n"
     ]
    }
   ],
   "source": [
    "pop_size = 1000 # Population\n",
    "n_embed=5 # Embedding dimension\n",
    "n_steps=1000 # Number of trials / steps\n",
    "max_rate = 0.2 # Maximum allowed spike probability\n",
    "\n",
    "# Generate two random vectors in [0,1]^N\n",
    "q,k = max_rate*np.random.random((2,n_embed,1)) # spike probabilties for key and query\n",
    "\n",
    "# Approximate scaled dot product using spikes (avg over population)\n",
    "output = approx_dot(q,k,pop_size,n_steps,comb_method='OR').mean(axis=0)\n",
    "\n",
    "sd = np.std(output,axis=0).item()\n",
    "avg = np.mean(output,axis=0).item()\n",
    "\n",
    "# Exact scaled dot product\n",
    "exact = (q.T@k/np.sqrt(n_embed)).item()\n",
    "\n",
    "# Error\n",
    "percent_error = np.abs((avg - exact) / exact) * 100\n",
    "angle = np.arccos(q.T@k/np.sqrt(q.T@q*k.T@k)).item()\n",
    "\n",
    "print(\"Exact scaled dot product: \", round(exact, 5))\n",
    "print(\"Approximation using spikes: \", round(avg, 5))\n",
    "print(\"Standard deviation: \", round(sd, 5))\n",
    "print(\"Percent error: \", round(percent_error, 5), \"%\")\n",
    "print(\"Angle: \", round(angle, 5), \" radians\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is the effect of sparsity?\n",
    "\n",
    "* The OR method works better for sparse spike trains, because the probability of collisions is low.\n",
    "* The SUM method works better for dense spike trains, because the variance of the sample spike rate is lower.\n",
    "* Let's compare these!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* As can be seen above, higher spike probabilities lead to less accurate approximations when combining channels using the inclusive OR method. \n",
    "* How does the maximum spike probability of the inputs affect this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19413/1211358094.py:19: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  error = np.sqrt(np.mean(diff**2)/np.mean(attn_scores**2))*100\n",
      "/tmp/ipykernel_19413/1211358094.py:25: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  error = np.sqrt(np.mean(diff**2)/np.mean(attn_scores**2))*100\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_embed=64 # Embedding dimension\n",
    "n_keys=100 # Number keys\n",
    "pop_size=1 # Population\n",
    "n_steps=5000 #50000 # Number of time steps\n",
    "\n",
    "OR_errors, SUM_errors = [], []\n",
    "max_rates = np.linspace(0.0,0.5,20)\n",
    "\n",
    "for max_rate in max_rates:\n",
    "\n",
    "    # Get keys/query and compute exact scaled dot products\n",
    "    q =  max_rate*np.random.random((n_embed,1)) # Query\n",
    "    K = max_rate*np.random.random((n_embed,n_keys)) # Keys\n",
    "    attn_scores = q.T@K/np.sqrt(n_embed) # Exact attention scores\n",
    "\n",
    "    # OR (avg error) \n",
    "    approx_attn_scores = approx_dot(q,K,pop_size,n_steps,comb_method='OR').mean(axis=(0,1))\n",
    "    diff = attn_scores - approx_attn_scores\n",
    "    error = np.sqrt(np.mean(diff**2)/np.mean(attn_scores**2))*100\n",
    "    OR_errors.append(error)\n",
    "\n",
    "    # SUM (avg error) \n",
    "    approx_attn_scores = approx_dot(q,K,pop_size,n_steps,comb_method='SUM').mean(axis=(0,1))\n",
    "    diff = attn_scores - approx_attn_scores\n",
    "    error = np.sqrt(np.mean(diff**2)/np.mean(attn_scores**2))*100\n",
    "    SUM_errors.append(error)\n",
    "\n",
    "plt.plot(max_rates,OR_errors,label='OR')\n",
    "plt.plot(max_rates,SUM_errors,label='SUM')\n",
    "plt.xlabel('Maximum allowed spike probability for key/query elements')\n",
    "plt.ylabel('Fractional error (%)')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Softmax\n",
    "\n",
    "Given a vector $\\bar{\\alpha}=(\\alpha_1,\\alpha_2,\\ldots,\\alpha_n)$, we define: $softmax(\\bar{\\alpha})=(e^{\\alpha_i-\\lambda})_{i\\in n}$, where $\\lambda=LSE(\\bar{\\alpha})=log(\\sum e^{\\alpha_i})$\n",
    "\n",
    "* Rather than trying to compute $\\lambda$ explicitly, we note that $\\lambda$ is the unique value that makes the softmax entries sum to 1. \n",
    "* We can adjust $\\lambda$ dynamically to make this true. \n",
    "* Therefore, the problem of compute softmax reduces to the problem of computing exponentials (using spikes).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "### Idea\n",
    "\n",
    "* Use the approximation: $e^{\\alpha-\\lambda}\\approx (1-\\frac{\\lambda-\\alpha}{N})^N$ for large $N$.\n",
    "* This reduces the problem. We just need to be able to generate spikes with probability $1-(\\lambda-\\alpha)/N$.\n",
    "* It may be useful to rewrite this expression as a more complex probability: $1-(\\lambda-\\alpha)/N=1-(\\lambda/N)(1-\\alpha/\\lambda)$\n",
    "* This expression is the probability that $A\\to B$, where $A$ and $B$ are independent Bernoulli r.v.s with probabilities $\\lambda/N$ and $\\alpha/\\lambda$, respectively.\n",
    "* Details: Let A and B be (independent) Bernoulli r.v.s Then $P(A\\to B) = P(\\lnot(A\\wedge\\lnot B))=1-P(A)[1-P(B)]$\n",
    "* Method: Flip $N$ coins each with probability $\\lambda/N$. For each coin that comes up heads, flip a coin with probability $\\alpha/\\lambda$. If all of the coins on this second round come up heads, we transmit a spike. The probability of spiking will be roughly $e^{\\alpha-\\lambda}$ if $N$ is sufficiently large. \n",
    "\n",
    "\n",
    "<!-- ![Exponential spiking](image1.svg) -->\n",
    "\n",
    "<img src=\"image1.svg\" alt=\"Description\" width=\"50%\"/>\n",
    "\n",
    "Note: The number of successes on the first round is distributed roughly like Poisson($\\lambda$) when $N$ is large. What if we actually use Poisson($\\lambda$)?\n",
    "\n",
    "Suppose that $k\\sim Poisson(\\lambda)$. What is the probability that $k$ independent Bernoulli($\\frac{\\alpha}{\\lambda}$) trials are all successful?\n",
    "\n",
    "Spike probability: $\\sum_k p(k)(\\frac{\\alpha}{\\lambda})^k=\\sum_k e^{-\\lambda}\\frac{\\lambda^k}{k!}(\\frac{\\alpha}{\\lambda})^k=e^{-\\lambda}\\sum_k\\frac{\\alpha^k}{k!}=e^{\\alpha-\\lambda}$.\n",
    "\n",
    "<!-- What if we replace the Poisson with a Binomial approximation (using N trials and success probability $\\lambda/N$)?\n",
    "\n",
    "Answer: $\\sum_k {N\\choose k}(\\frac{\\lambda}{N})^k(1-\\frac{\\lambda}{N})^{N-k}(\\frac{\\alpha}{\\lambda})^k=(1-\\frac{\\lambda}{N})^N\\sum_k{N \\choose k}(\\frac{\\alpha}{N-\\lambda})^k$ -->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact exponential:  0.3678794411714424\n",
      "Approximation using spikes:  0.36729\n",
      "Error:  0.16022672252774453 %\n"
     ]
    }
   ],
   "source": [
    "# Basic exponential\n",
    "\n",
    "T=100000\n",
    "alpha = 5*np.random.random()\n",
    "lam = alpha + 1\n",
    "\n",
    "# Exact exponential\n",
    "exact = np.exp(alpha-lam)\n",
    "print(\"Exact exponential: \",exact)\n",
    "\n",
    "# Approximate exponential using spikes\n",
    "spikes = ExpBernoulliAlt(alpha,lam,T)\n",
    "approx = np.mean(spikes)\n",
    "print(\"Approximation using spikes: \",approx)\n",
    "\n",
    "# Error\n",
    "error = np.abs((exact-approx)/exact)\n",
    "print(\"Error: \",100*error,'%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total probability (this should be 1):  5.3726199999999995\n",
      "Total probability (this should be 1):  0.60249\n",
      "Total probability (this should be 1):  0.73601\n",
      "Total probability (this should be 1):  0.83223\n",
      "Total probability (this should be 1):  0.9101400000000001\n",
      "Total probability (this should be 1):  0.9569000000000001\n",
      "Total probability (this should be 1):  0.97103\n",
      "Total probability (this should be 1):  0.9895\n",
      "Total probability (this should be 1):  0.99464\n",
      "Total probability (this should be 1):  0.9938600000000001\n",
      "\n",
      "\n",
      "Original vector:  [0.878 0.021 0.527 0.209 0.354 0.89  0.071 0.586]\n",
      "Exact softmax:  [0.184 0.078 0.129 0.094 0.109 0.186 0.082 0.137]\n",
      "Approximation using spike:  [0.18197, 0.07796, 0.13017, 0.09389, 0.10831, 0.18284, 0.08066, 0.13806]\n",
      "Error:  1.1496181170240176 %\n"
     ]
    }
   ],
   "source": [
    "# Basic softmax\n",
    "n_embed = 8\n",
    "dt = 0.5\n",
    "N=10\n",
    "T=100000\n",
    "\n",
    "alpha = np.random.random(n_embed) # alpha\n",
    "exact = softmax(np.array(alpha))\n",
    "lam = max(alpha) # Initial value for lambda\n",
    "\n",
    "# Loops\n",
    "for _ in range(N): # Iterate to adjust lambda\n",
    "    spikes = [ExpBernoulliAlt(x,lam,T) for x in alpha] # Get spike trains for each channel\n",
    "    approx = [np.mean(z) for z in spikes]\n",
    "    total_probability = np.sum(approx)\n",
    "    print('Total probability (this should be 1): ', total_probability)\n",
    "    lam += dt*(total_probability-1) # How the fuck did copilot guess this line from \"lam += dt*\"?????!!!!!!!!!!\n",
    "error = np.sqrt(np.mean((exact-approx)**2)/np.mean(exact**2))*100\n",
    "\n",
    "print('\\n')\n",
    "print('Original vector: ',alpha)\n",
    "print('Exact softmax: ',exact)\n",
    "print('Approximation using spike: ',approx)\n",
    "print('Error: ',error,'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention scores:  [[0.078 0.108 0.092 0.12  0.122]]\n",
      "Exact exponentials:  [[0.398 0.41  0.403 0.415 0.415]]\n",
      "Approximation using spikes:  [0.39  0.425 0.428 0.409 0.415]\n",
      "Error:  3.3229223179537892 %\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(precision=3)\n",
    "n_embed=4 # Embedding dimension\n",
    "n_keys=5 # Number keys\n",
    "M=1000\n",
    "T=20000\n",
    "max_rate = 0.5\n",
    "lam=1.0\n",
    "\n",
    "\n",
    "# Get keys/query and compute exact scaled dot products\n",
    "q =  max_rate*np.random.random((n_embed,1)) # Query\n",
    "K = max_rate*np.random.random((n_embed,n_keys)) # Keys\n",
    "attn_scores = q.T@K/np.sqrt(n_embed) # Exact attention scores\n",
    "print(\"Attention scores: \",attn_scores)\n",
    "\n",
    "exact = np.exp(attn_scores-lam)\n",
    "print(\"Exact exponentials: \",exact)\n",
    "\n",
    "A_spikes = np.random.random((M,T,n_keys)) < lam/T\n",
    "B_spikes=approx_fn(q/lam,K,M*T,comb_method='OR').reshape(M,T,-1)\n",
    "attn_score_spikes = ~A_spikes | B_spikes # AND\n",
    "\n",
    "approx = np.mean(np.all(attn_score_spikes,axis=1),axis=0)\n",
    "error = np.sqrt(np.mean((approx-exact)**2)/np.mean(exact**2))\n",
    "print(\"Approximation using spikes: \",approx)\n",
    "print(\"Error: \",100*error,'%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## We can avoid doing the sum in the dot products...\n",
    "\n",
    "* Basic idea: $exp(q\\cdot k-\\lambda) = exp(\\sum_N q_is_i-\\lambda)=\\prod_N exp(r_is_i-\\lambda/N)$\n",
    "* This avoids summing spike trains, so it's okay if $r_is_i$ is close to 1 (i.e. we don't need sparsity).\n",
    "* We can do $N\\times K$ initial coin flips, each with probability $\\lambda/N$. \n",
    "* Whenever coin $(n,k)$ lands heads up, we perform a second coin flip with probability $r_ns_n/\\lambda\\sqrt{N}$.\n",
    "* If all of the second round coin flips come up heads, we transmit a spike.\n",
    "* Spike probability: $\\prod_n\\prod_k 1-\\frac{\\lambda}{K}(1-r_ns_n/\\lambda\\sqrt{N}) = \\prod_n [1-\\frac{\\lambda}{K}(1-r_ns_n/\\lambda\\sqrt{N})]^K\\prod_n [1-\\frac{\\lambda-r_ns_n/\\sqrt{N}}{K})]^K\\approx \\prod_n e^{r_ns_n/\\sqrt{N}-\\lambda}=e^{r\\cdot s/\\sqrt{N}-N\\lambda}$\n",
    "\n",
    "##### Below is a full exmaple, including adjusting $\\lambda$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total probability:  0.131\n",
      "Lambda:  1\n",
      "Total probability:  0.572\n",
      "Lambda:  0.6367000000000002\n",
      "Total probability:  0.9\n",
      "Lambda:  0.5198000000000002\n",
      "Total probability:  0.994\n",
      "Lambda:  0.4934000000000001\n",
      "Total probability:  1.027\n",
      "Lambda:  0.49170000000000014\n",
      "Total probability:  1.045\n",
      "Lambda:  0.49110000000000004\n",
      "Total probability:  0.923\n",
      "Lambda:  0.5037\n",
      "Total probability:  0.988\n",
      "Lambda:  0.49230000000000007\n",
      "Total probability:  1.04\n",
      "Lambda:  0.4903000000000001\n",
      "Total probability:  1.0150000000000001\n",
      "Lambda:  0.4934000000000001\n",
      "\n",
      "Attention scores:  [[0.414 0.294 0.567 0.362 0.166]]\n",
      "Exact softmax:  [[0.209 0.185 0.244 0.199 0.163]]\n",
      "Approximation using spikes:  [0.203 0.19  0.244 0.195 0.163]\n",
      "Error:  1.8828207345448267 %\n",
      "Total probability:  0.9951\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(precision=3)\n",
    "n_embed=4 # Embedding dimension\n",
    "n_keys=5 # Number keys\n",
    "N=1000 # How many coins to flip in first phase\n",
    "T=200 \n",
    "dt=0.1\n",
    "lam=1\n",
    "max_rate = 1.0\n",
    "\n",
    "# Get keys/query and compute exact scaled dot products\n",
    "q =  max_rate*np.random.random((n_embed,1)) # Query\n",
    "K = max_rate*np.random.random((n_embed,n_keys)) # Keys\n",
    "\n",
    "# Get correct lambda value\n",
    "##################################################################################\n",
    "\n",
    "for i in range(50):\n",
    "    # Create spike arrays based on the values in q and K\n",
    "    q_spikes = np.random.random((N,T,n_embed,1))<q\n",
    "    K_spikes = np.random.random((N,T,n_embed,n_keys))<K\n",
    "    scaling_spikes = np.random.random((N,T,n_embed,n_keys)) < 1/(lam*np.sqrt(n_embed)) # K x N matrix of boolean mask\n",
    "    # scaling_spikes = np.random.random((1,T,n_embed,n_keys)) < 1/lam*np.sqrt(n_embed) # K x N matrix of boolean mask\n",
    "    A_spikes = np.random.random((N,T,n_embed,n_keys)) < lam/T\n",
    "    B_spikes = q_spikes & K_spikes & scaling_spikes\n",
    "    approx_spikes = ~A_spikes | B_spikes # A->B \n",
    "\n",
    "    approx = np.mean(np.all(approx_spikes,axis=(1,2)),axis=0)\n",
    "    if i%5==0:\n",
    "        print(\"Total probability: \",np.sum(approx))\n",
    "        print(\"Lambda: \",lam)\n",
    "    lam -= dt*(1-np.sum(approx)) # Adjust lambda\n",
    "\n",
    "###################################################################################\n",
    "\n",
    "# Softmax\n",
    "####################################################################################\n",
    "N=10000\n",
    "T=1000\n",
    "\n",
    "attn_scores = q.T@K/np.sqrt(n_embed) # Exact attention scores\n",
    "print(\"\\nAttention scores: \",attn_scores)\n",
    "exact = softmax(attn_scores)\n",
    "print(\"Exact softmax: \",exact)\n",
    "\n",
    "# Create spike arrays based on the values in q and K\n",
    "q_spikes = np.random.random((N,T,n_embed,1))<q\n",
    "K_spikes = np.random.random((N,T,n_embed,n_keys))<K\n",
    "# scaling_spikes = np.random.random((N,T,n_embed,n_keys)) < 1/(lam*np.sqrt(n_embed)) # K x N matrix of boolean mask\n",
    "scaling_spikes = np.random.random((N,T,n_embed,n_keys)) < 1/(lam*np.sqrt(n_embed)) # K x N matrix of boolean mask\n",
    "A_spikes = np.random.random((N,T,n_embed,n_keys)) < lam/T\n",
    "B_spikes = q_spikes & K_spikes & scaling_spikes\n",
    "approx_spikes = ~A_spikes | B_spikes # A -> B\n",
    "\n",
    "approx = np.mean(np.all(approx_spikes,axis=(1,2)),axis=0)\n",
    "error = np.sqrt(np.mean((approx-exact)**2)/np.mean(exact**2))\n",
    "print(\"Approximation using spikes: \",approx)\n",
    "print(\"Error: \",100*error,'%')\n",
    "print(\"Total probability: \",np.sum(approx))\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
