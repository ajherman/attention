{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "np.set_printoptions(precision=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    y = np.exp(x)\n",
    "    return y/np.sum(y)\n",
    "\n",
    "\n",
    "def ExpBernoulliAlt(c,lam,size=None,method='poisson',N=1000): # returns True with probability $e^{c-lam}$\n",
    "    if size is None: \n",
    "        size = (1,)\n",
    "    if type(size) is int:\n",
    "        size = (size,)\n",
    "    elif type(size) is not tuple:\n",
    "        raise TypeError(\"size must be int or tuple\")\n",
    "    \n",
    "    if method == 'poisson':\n",
    "        M=np.random.poisson(lam=lam,size=size) # Get integers from poisson(lam).\n",
    "    elif method == 'binomial': # N is number to flip\n",
    "        M=np.random.binomial(N,lam/N,size=size)\n",
    "\n",
    "    # if type(c) is float:\n",
    "    spikes = np.random.random(size)<np.power(c/lam,M) # Spike if all K trials are successful\n",
    "    return spikes\n",
    "\n",
    "def approx_fn(q,K,T,comb_method='OR'):\n",
    "    n_embed,n_keys = K.shape\n",
    "\n",
    "    # Create spike arrays based on the values in q and K\n",
    "    q_spikes = np.random.random((T,n_embed,1))<q\n",
    "    K_spikes = np.random.random((T,n_embed,n_keys))<K\n",
    "    scaling_spikes = np.random.random((T,n_embed,n_keys)) < 1/np.sqrt(n_embed) # K x N matrix of boolean mask\n",
    "    attn_score_spikes = q_spikes & K_spikes & scaling_spikes # AND\n",
    "\n",
    "    # Summing works better, but OR can work well if input rates are low\n",
    "    if comb_method == 'OR':\n",
    "        attn_score_spikes = np.any(attn_score_spikes, axis=1) # OR\n",
    "    elif comb_method == 'SUM':\n",
    "        attn_score_spikes = np.sum(attn_score_spikes, axis=1) # SUM\n",
    "    else:\n",
    "        assert(0)\n",
    "    approx_attn_scores = np.mean(attn_score_spikes,axis=0) # Get spike rates\n",
    "    return approx_attn_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We take a random query, $q$ and key, $k$, and try to compute their scaled dot product using spikes. \n",
    "\n",
    "Basic method: \n",
    "\n",
    "* Create spikes trains with rates proportional to the elements of $k$, $q$ and $1/\\sqrt{N}$ (where $N$ is the lenght of $q,k$).\n",
    "* Use AND gates to combine this, yields spike trains with rates proportional to $q_ik_i/\\sqrt{N}$.\n",
    "* Sum these spike trains together. If each individual spike train is sufficiently sparse, we can do this using an inclusive OR.\n",
    "\n",
    "Challenges: \n",
    "* What if we want to using key or query vectors that are not in $[0,1]^N$?\n",
    "* What if input spike trains are not very sparse? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (1,5) and (1,5) not aligned: 5 (dim 1) != 1 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_241409/2714511512.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mapprox\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapprox_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# Exact scaled dot product\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mexact\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0mpercent_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapprox\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mexact\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mexact\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mangle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marccos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (1,5) and (1,5) not aligned: 5 (dim 1) != 1 (dim 0)"
     ]
    }
   ],
   "source": [
    "N=5 # Embedding dimension\n",
    "T=10000 # Number of trials / steps\n",
    "max_rate = 1.0 # Maximum allowed spike probability\n",
    "\n",
    "# Generate two random vectors in [0,1]^N\n",
    "q,k = max_rate*np.random.random((2,N,1)) # spike probabilties for key and query\n",
    "\n",
    "# # Get spikes for both vectors and for the scaling mask\n",
    "# q_spikes = np.random.random((K,N)) < q   # K x N matrix of boolean spikes for neuron x\n",
    "# k_spikes = np.random.random((K,N)) < k   # K x N matrix of boolean spikes for neuron y\n",
    "# scaling_spikes = np.random.random((K,N)) < 1/np.sqrt(N) # K x N matrix of boolean mask\n",
    "\n",
    "# # Compute scaled dot product with spikes (approximate)\n",
    "# z = q_spikes & k_spikes & scaling_spikes # K vector of boolean ANDs, then averaged over neurons\n",
    "# # output_spike_train = np.any(z, axis=1) # K vector of boolean ORs\n",
    "# output_spike_train = np.sum(z, axis=1) # K vector of boolean ORs\n",
    "# approx = np.mean(output_spike_train) # average over K trials\n",
    "approx = approx_fn(q,k,T)\n",
    "# Exact scaled dot product\n",
    "exact = np.dot(q,k)/np.sqrt(N)\n",
    "percent_error = np.abs(approx - exact) / exact * 100\n",
    "angle = np.arccos(np.dot(q,k)/np.sqrt(np.dot(q,q)*np.dot(k,k)))\n",
    "\n",
    "print(\"Exact scaled dot product: \", round(exact, 5))\n",
    "print(\"Approximate scaled dot product: \", round(approx, 5))\n",
    "print(\"Percent error: \", round(percent_error, 5), \"%\")\n",
    "print(\"Angle: \", round(angle, 5), \" radians\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Spiking dot products. Examples of dot products computed approximately with AND and OR gates.\n",
    "\n",
    "One challenge of spiking representing values with spike probabilities is that we can only (naturally) represent positive values. However, our dot product values are going to passed into a softmax, which is invariant under adding a constant to all inputs. Therefore, adding any fixed vector to all keys will not change the outcome of the softmax.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Spiking dot products:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact attention scores: \n",
      " [[0.19 0.2  0.09 0.2  0.17 0.17 0.18 0.27 0.19 0.2 ]]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'numpy.float64' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_241409/3299105650.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m# approx_attn_scores = np.mean(attn_score_spikes,axis=0) # Get spike rates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mapprox_attn_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapprox\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Approximate attenion scores: \\n\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mapprox_attn_scores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'numpy.float64' object is not callable"
     ]
    }
   ],
   "source": [
    "# With a small embedding dimension we can fully explore space of possible vectors. \n",
    "# With a large embedding dimension, dot products tend towards a certain value. \n",
    "\n",
    "n_embed=5 # Embedding dimension\n",
    "n_keys=10 # Number keys\n",
    "T=50000 # Number of time steps\n",
    "\n",
    "# q,K = np.abs(np.random.normal(0, 1./10, (n_embed,1))),np.abs(np.random.normal(0, 1./10, (n_embed,n_keys))) # spike probability of each neuron\n",
    "\n",
    "max_rate = 0.5 # The maximum allow spike rate for inputs\n",
    "q =  max_rate*np.random.random((n_embed,1)) # Query\n",
    "K = max_rate*np.random.random((n_embed,n_keys)) # Keys\n",
    "\n",
    "# Baseline using continuous values\n",
    "attn_scores = q.T@K/np.sqrt(n_embed)\n",
    "print(\"Exact attention scores: \\n\",attn_scores)\n",
    "\n",
    "# # Create spike arrays based on the values in q and K\n",
    "# q_spikes = np.random.random((T,n_embed,1))<q\n",
    "# K_spikes = np.random.random((T,n_embed,n_keys))<K\n",
    "# scaling_mask = np.random.random((T,n_embed,n_keys)) < 1/np.sqrt(n_embed) # K x N matrix of boolean mask\n",
    "# attn_score_spikes = q_spikes & K_spikes & scaling_mask # AND\n",
    "\n",
    "# # Summing works better, but OR can work well if input rates are low\n",
    "# attn_score_spikes = np.any(attn_score_spikes, axis=1) # OR\n",
    "# # attn_score_spikes = np.sum(attn_score_spikes, axis=1) # SUM\n",
    "\n",
    "# approx_attn_scores = np.mean(attn_score_spikes,axis=0) # Get spike rates\n",
    "approx_attn_scores = approx(q,K,T)\n",
    "print(\"Approximate attenion scores: \\n\",approx_attn_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_212748/2810556129.py:18: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  error = np.sqrt(np.mean(diff**2)/np.mean(attn_scores**2))\n",
      "/tmp/ipykernel_212748/2810556129.py:24: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  error = np.sqrt(np.mean(diff**2)/np.mean(attn_scores**2))\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_embed=64 # Embedding dimension\n",
    "n_keys=100 # Number keys\n",
    "T=50000 # Number of time steps\n",
    "\n",
    "OR_errors, SUM_errors = [], []\n",
    "max_rates = np.linspace(0.0,0.5,20)\n",
    "\n",
    "for max_rate in max_rates:\n",
    "\n",
    "    # Get keys/query and compute exact scaled dot products\n",
    "    q =  max_rate*np.random.random((n_embed,1)) # Query\n",
    "    K = max_rate*np.random.random((n_embed,n_keys)) # Keys\n",
    "    attn_scores = q.T@K/np.sqrt(n_embed) # Exact attention scores\n",
    "\n",
    "    # OR (avg error) \n",
    "    approx_attn_scores = approx(q,K,T,comb_method='OR')\n",
    "    diff = attn_scores - approx_attn_scores\n",
    "    error = np.sqrt(np.mean(diff**2)/np.mean(attn_scores**2))\n",
    "    OR_errors.append(error)\n",
    "\n",
    "    # SUM (avg error) \n",
    "    approx_attn_scores = approx(q,K,T,comb_method='SUM')\n",
    "    diff = attn_scores - approx_attn_scores\n",
    "    error = np.sqrt(np.mean(diff**2)/np.mean(attn_scores**2))\n",
    "    SUM_errors.append(error)\n",
    "\n",
    "plt.plot(max_rates,OR_errors,label='OR')\n",
    "plt.plot(max_rates,SUM_errors,label='SUM')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Softmax: version where we passed attentions scores directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.22937e-03 2.37173e-03 4.66050e-04 2.11442e-03 3.80521e-05]]\n",
      "[2.22937e-03 2.37173e-03 4.66050e-04 2.11442e-03 3.80521e-05]\n",
      "[0.20016 0.20019 0.1998  0.20013 0.19972]\n",
      "[0.20054, 0.20006, 0.19982, 0.19888, 0.20096]\n"
     ]
    }
   ],
   "source": [
    "n_embed=1 # Embedding dimension\n",
    "n_keys=5 # Number keys\n",
    "# T=50000 # Number of time steps\n",
    "max_rate = 0.1\n",
    "\n",
    "# Get keys/query and compute exact scaled dot products\n",
    "q =  max_rate*np.random.random((n_embed,1)) # Query\n",
    "K = max_rate*np.random.random((n_embed,n_keys)) # Keys\n",
    "attn_scores = q.T@K/np.sqrt(n_embed) # Exact attention scores\n",
    "np.set_printoptions(precision=5)\n",
    "print(attn_scores)\n",
    "# lam = max(attn_scores)\n",
    "# print(attn_scores.shape)\n",
    "# assert(0)\n",
    "\n",
    "N=500 # Population size\n",
    "T=50000 # Number of trials\n",
    "\n",
    "c = attn_scores[0] #[3,0,1,2,3,7] # Inputs to softmax\n",
    "dt = 0.01\n",
    "lam = max(c)\n",
    "\n",
    "for _ in range(N):\n",
    "    li = [0 for _ in c]\n",
    "    for i,x in enumerate(c):\n",
    "        spikes = ExpBernoulliAlt(x,lam,T)\n",
    "        li[i] = spikes\n",
    "    total_mean_spikes = np.sum([np.mean(z) for z in li])\n",
    "    lam += dt*(total_mean_spikes-1) # How the fuck did copilot guess this line from \"lam += dt*\"?????!!!!!!!!!!\n",
    "    # print(total_mean_spikes)\n",
    "print(c)\n",
    "print(softmax(np.array(c)))\n",
    "print([np.mean(z) for z in li])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_embed=1 # Embedding dimension\n",
    "n_keys=5 # Number keys\n",
    "T=50000 # Number of time steps\n",
    "max_rate = 0.1\n",
    "\n",
    "# Get keys/query and compute exact scaled dot products\n",
    "q =  max_rate*np.random.random((n_embed,1)) # Query\n",
    "K = max_rate*np.random.random((n_embed,n_keys)) # Keys\n",
    "attn_scores = q.T@K/np.sqrt(n_embed) # Exact attention scores\n",
    "np.set_printoptions(precision=5)\n",
    "print(attn_scores)\n",
    "# lam = max(attn_scores)\n",
    "# print(attn_scores.shape)\n",
    "# assert(0)\n",
    "\n",
    "N=500 # Population size\n",
    "K=10000 # Number of trials\n",
    "\n",
    "c = attn_scores[0] #[3,0,1,2,3,7] # Inputs to softmax\n",
    "dt = 0.01\n",
    "lam = max(c)\n",
    "\n",
    "for _ in range(N):\n",
    "    li = [0 for _ in c]\n",
    "    for i,x in enumerate(c):\n",
    "        spikes = ExpBernoulliAlt(x,lam,K)\n",
    "        li[i] = spikes\n",
    "    total_mean_spikes = np.sum([np.mean(z) for z in li])\n",
    "    lam += dt*(total_mean_spikes-1) # How the fuck did copilot guess this line from \"lam += dt*\"?????!!!!!!!!!!\n",
    "    # print(total_mean_spikes)\n",
    "print(c)\n",
    "print(softmax(np.array(c)))\n",
    "print([np.mean(z) for z in li])\n",
    "\n",
    "def ExpBernoulliAlt(c,lam,size=None,method='poisson',N=1000): # returns True with probability $e^{c-lam}$\n",
    "    if size is None: \n",
    "        size = (1,)\n",
    "    if type(size) is int:\n",
    "        size = (size,)\n",
    "    elif type(size) is not tuple:\n",
    "        raise TypeError(\"size must be int or tuple\")\n",
    "    \n",
    "    if method == 'poisson':\n",
    "        M=np.random.poisson(lam=lam,size=size) # Get integers from poisson(lam).\n",
    "    elif method == 'binomial': # N is number to flip\n",
    "        M=np.random.binomial(N,lam/N,size=size)\n",
    "\n",
    "    # if type(c) is float:\n",
    "    spikes = np.random.random(size)<np.power(c/lam,M) # Spike if all K trials are successful\n",
    "    return spikes\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Softmax: Version where we pass spikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "attn_score_spikes = np.any(attn_score_spikes, axis=1)\n",
    "# attn_score_spikes = np.sum(attn_score_spikes, axis=1)\n",
    "\n",
    "approx_attn_scores = np.mean(attn_score_spikes, axis=0)\n",
    "\n",
    "# Compare\n",
    "print(\"Approx: \",approx_attn_scores)\n",
    "print(\"Exact: \",attn_scores)\n",
    "print(\"Error :\",1-np.mean(approx_attn_scores/attn_scores))\n",
    "print(\"Ratios: \",approx_attn_scores/attn_scores)\n",
    "\n",
    "# Scatter plot\n",
    "# plt.scatter(attn_scores, approx_attn_scores)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Exact Attention Scores')\n",
    "plt.ylabel('Approximate Attention Scores')\n",
    "plt.title('Exact vs Approximate Attention Scores')\n",
    "\n",
    "# Scatter plot\n",
    "plt.scatter(attn_scores, approx_attn_scores)\n",
    "# Add y=x line\n",
    "plt.plot([np.min(attn_scores), np.max(attn_scores)], [np.min(attn_scores), np.max(attn_scores)], color='black', linestyle='dashed')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New idea that might work well. So, let's say that N is the dimension of the keys and queries. When we first dot the key and query spikes, we do AND operations and the output is a vector of lenghth N. The sum of this vector has the dot product as its expected value. However, what if instead of doing a sum or an OR, we flip N coins to get an approx to Poisson(lambda) and then we transmit a spike if all of the coins that came up heads were also heads in the AND vector. I.e. all of the coins that came of heads were also heads in both the key and the query vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approx : [0.096 0.094 0.084 0.082 0.092 0.084 0.082 0.088 0.086 0.102 0.094 0.088\n",
      " 0.098 0.09  0.094 0.092 0.102 0.09  0.086 0.086]\n",
      "Exact:  [[0.1        0.07591026 0.07393153 0.07666985 0.07402809 0.07000596\n",
      "  0.07088452 0.07685    0.07300686 0.07437543 0.07594129 0.0762364\n",
      "  0.07551446 0.07362136 0.07728051 0.0743656  0.07741174 0.07859786\n",
      "  0.07764515 0.07697252]]\n",
      "Error : -0.1875686704183599\n",
      "Ratios:  [[0.96       1.23830431 1.13618638 1.06952086 1.24277148 1.1998978\n",
      "  1.15681105 1.14508782 1.17797154 1.37142056 1.23779838 1.15430417\n",
      "  1.29776473 1.22247129 1.21634811 1.23713111 1.31762962 1.14506933\n",
      "  1.10760304 1.11728183]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "outs=[]\n",
    "for trial in range(500):\n",
    "    T=np.random.poisson(lam=lam,size=1)[0] # Get integers from poisson(lam). Number of successes needs\n",
    "    # Spiking approximation\n",
    "    q_spikes = np.random.random((T,dm,1))<q\n",
    "    K_spikes = np.random.random((T,dm,n_keys))<K\n",
    "    mask = np.random.random((T,dm,n_keys)) < 1/np.sqrt(dm) # K x N matrix of boolean mask\n",
    "    attn_score_spikes = q_spikes & K_spikes & mask\n",
    "    sum_spikes = np.any(attn_score_spikes, axis=1)\n",
    "    out = np.all(sum_spikes, axis=0,keepdims=True)\n",
    "    # flips = np.random.random((T,M))<lam/T\n",
    "    # lam_invert = np.random.random((T,M))<1/lam\n",
    "    # # ~flips | (sum_spikes & lam_invert)\n",
    "    # out = np.all(~flips | (sum_spikes & lam_invert), axis=0,keepdims=True)\n",
    "    outs.append(out)\n",
    "outs = np.concatenate(outs,axis=0)\n",
    "mean_out = np.mean(outs,axis=0)\n",
    "print(\"Approx :\",mean_out)\n",
    "print(\"Exact: \",attn_scores)\n",
    "print(\"Error :\",1-np.mean(mean_out/attn_scores))\n",
    "print(\"Ratios: \",mean_out/attn_scores)\n",
    "\n",
    "\n",
    "# q_spikes = np.random.random((T,N,1))<q\n",
    "# K_spikes = np.random.random((T,N,M))<K\n",
    "# mask = np.random.random((T,N,M)) < 1/np.sqrt(N) # K x N matrix of boolean mask\n",
    "# score_spikes = q_spikes & K_spikes & mask\n",
    "# sum_spikes = np.any(score_spikes, axis=1)\n",
    "# flips = np.random.random((T,M))<lam/T\n",
    "# lam_invert = np.random.random((T,M))<1/lam\n",
    "# # ~flips | (sum_spikes & lam_invert)\n",
    "# out = np.all(~flips | (sum_spikes & lam_invert), axis=0,keepdims=True)\n",
    "# # print(np.shape(out))\n",
    "# print(out/attn_scores)\n",
    "# # sum_spikes_reduced = sum_spikes @\n",
    "# # mean_spikes = np.mean(sum_spikes, axis=0,keepdims=True)\n",
    "# # print(np.shape(sum_spikes))\n",
    "# print(np.shape(attn_scores))\n",
    "# error = mean_spikes-attn_scores\n",
    "# print(np.sqrt(np.mean(error**2)))\n",
    "# print(np.sqrt(np.mean(attn_scores**2)))\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
