
Inactive Modules:
  1) miniconda3


Activating Modules:
  1) miniconda3/py39_4.12.0

Epoch 0:   0%|          | 0/1115394 [00:00<?, ?it/s]/vast/home/ajherman/miniconda3/envs/pytorch/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/vast/home/ajherman/miniconda3/envs/pytorch/lib/python3.11/site-packages/transformers/generation/utils.py:1262: UserWarning: Input length of input_ids is 171, but `max_length` is set to 50. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.
  warnings.warn(
Epoch 0:   0%|          | 0/1115394 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "/vast/home/ajherman/learning/gpt_example.py", line 66, in <module>
    sample_output = model.generate(inputs, max_length=50, num_beams=5, temperature=0.8)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/vast/home/ajherman/miniconda3/envs/pytorch/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/vast/home/ajherman/miniconda3/envs/pytorch/lib/python3.11/site-packages/transformers/generation/utils.py", line 1675, in generate
    return self.beam_search(
           ^^^^^^^^^^^^^^^^^
  File "/vast/home/ajherman/miniconda3/envs/pytorch/lib/python3.11/site-packages/transformers/generation/utils.py", line 2971, in beam_search
    batch_beam_size, cur_len = input_ids.shape
    ^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: too many values to unpack (expected 2)
